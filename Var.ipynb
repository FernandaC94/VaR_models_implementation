{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "from pandas_datareader import data as pdr\n",
    "from scipy.stats import norm,binom,chi2\n",
    "from scipy.linalg import cholesky\n",
    "import matplotlib.pyplot as plt \n",
    "import datetime as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBTAINING THE DATA FROM YAHOO FINANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(tickers_list,start,end):\n",
    "    \"\"\"inputs: function whose inputs are the list of the ticks for\n",
    "    the assets we want to include in our portfolio,the start\n",
    "    and end date of analysis.\n",
    "    outputs:\n",
    "     -data: DataFrame of market variables or asset prices\n",
    "     -returns: DataFrame of the percentage change or returns of the market variable\n",
    "     -meanReturns: the mean of the returns for each market variable\n",
    "     -covMatrix: the covariance matrix of the market variables\"\"\"\n",
    "    data=pdr.get_data_yahoo(tickers_list, start, end)['Close']\n",
    "    data.reset_index(inplace=True)\n",
    "    dates=data.loc[1:,'Date']\n",
    "    data.drop(columns=['Date'],inplace=True)\n",
    "    returns=data.pct_change()\n",
    "    returns.dropna(inplace=True)\n",
    "    returns.reset_index(inplace=True, drop=True)\n",
    "    meanReturns=returns.mean(axis=0)\n",
    "    covMatrix=returns.cov()\n",
    "\n",
    "    return data,returns,dates,meanReturns,covMatrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOLATILITY ESTIMATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing Garch(1,1) and M-Garch(1,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Garch1_1:\n",
    "    \"\"\"This is a class that calculate GARCH(1,1) optimal parameters c, a, b for a market variable returns and calculate \n",
    "    the conditional variances with the optimal parameters \"\"\"\n",
    "\n",
    "    def __init__(self, returns):\n",
    "        self.returns = np.array(returns)\n",
    "        self.n=len(self.returns)\n",
    "        self.params_opt = self.garch_opt()  #extracting the optimal parameters c, a, b\n",
    "    \n",
    "\n",
    "    def garch_variance(self, parameters):\n",
    "        \"Returns the variance of a GARCH(1,1) process with the parameters passed\"\n",
    "        \n",
    "        # specifying parameters\n",
    "        c = parameters[0]\n",
    "        a = parameters[1]\n",
    "        b = parameters[2]\n",
    "\n",
    "       \n",
    "        # Initializing an empty array for the varainces in each day\n",
    "        var_list = np.zeros(self.n)\n",
    "\n",
    "        # calculating the long-run variance\n",
    "        Vl=c/(1 - a - b)   \n",
    "        var_list[0]=Vl\n",
    "        #calculating variance \n",
    "        for i in range(1,self.n):\n",
    "                var_list[i] = c + a * self.returns[i-1]**2 + b * var_list[i-1]\n",
    "        return var_list\n",
    "        \n",
    "    def garch_loglikelihood(self, parameters):\n",
    "        \" Here we define the log likelihood function to be optimize\"\n",
    "\n",
    "        var = self.garch_variance(parameters)\n",
    "        logL = np.sum(-np.log(var) - self.returns**2 / var)\n",
    "        return -logL\n",
    "    \n",
    "    def garch_opt(self):\n",
    "        \"returns the optimal parameters minimizing the log likelihood function\"\n",
    "        #defining constraints\n",
    "        constraint=lambda parameters: (1-1e-8)-parameters[1]-parameters[2] \n",
    "        con={'type':'ineq','fun':constraint}\n",
    "        opt_params=scipy.optimize.minimize(self.garch_loglikelihood,(np.var(self.returns),0.2,0.6),bounds=((1e-10,1),(1e-10,1),(1e-10,1)),constraints=con)\n",
    "        return opt_params.x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCC_garch:\n",
    "    \"\"\"This class implement the DCC m-GARCH(1,1) model for a multi-asset portfolio. This is the second step of the DCC model \n",
    "    where we optimize the value of  the parameters a and b. The first step of the DCC model is obtaining the\n",
    "    parameters c,a,b for each of the assets in the portfolio\"\"\"\n",
    "    \n",
    "    def __init__(self,returns):\n",
    "        self.returns=np.array(returns)\n",
    "        \n",
    "        \n",
    "    def m_garch_vol(self):\n",
    "        \" fetch the conditional variance for each asset portfolio in each day n and storage the squared root of them in a matrix D of size\\\n",
    "          nxJ where n is the number of days in our sample and J is the number of assets in our portfolio. These are the conditional\\\n",
    "          volatilities\"\n",
    "        \n",
    "        row,col=self.returns.shape #number of data points and assets respectively.\n",
    "        S=np.zeros([row,col])  ##place holder of the variances for each market variable\n",
    "        \n",
    "        for i in range(col):\n",
    "            garch=Garch1_1(self.returns[:,i])\n",
    "            opt_params=garch.garch_opt()\n",
    "            \n",
    "            S[:,i]=np.sqrt(garch.garch_variance(opt_params))\n",
    "        \n",
    "        return S\n",
    "    \n",
    "\n",
    "    def m_garch_loglikilihood(self,parameters): \n",
    "        \"Especification of the log-likelihood function to estimate \\\n",
    "        parameter a and b\"\n",
    "        \n",
    "        a=parameters[0]\n",
    "        b=parameters[1]\n",
    "        row,col=self.returns.shape\n",
    "\n",
    "        S=self.m_garch_vol() # creating the S matrix with all the standard deviations\n",
    "        Q_bar=np.cov(self.returns.reshape(col,row))#unconditional covariance .We approximate it by the sample covariance\n",
    "        \n",
    "        Q=np.zeros((row,col,col))\n",
    "        R=np.zeros((row,col,col))\n",
    "        H=np.zeros((row,col,col))\n",
    "\n",
    "        logL=0\n",
    "\n",
    "        Q[0]=np.matmul(self.returns[0].T,self.returns[0])    \n",
    "\n",
    "        for i in range(1,row): #for each day\n",
    "            D_n=np.diag(S[i,:])\n",
    "            D_n_inv=np.linalg.inv(D_n)\n",
    "            resd=D_n_inv*self.returns[i,:].T  #calculating the standardized residuals\n",
    "            Q[i]=(1-a-b)*Q_bar+a*resd*resd.T +b*Q[i-1]      #calculating matrix Q\n",
    "            Q_star_inv=np.linalg.inv(np.sqrt(np.diag(np.diag(Q[i]))))\n",
    "            R[i]=np.dot(Q_star_inv,np.dot(Q[i],Q_star_inv))\n",
    "            H[i]=np.dot(D_n,np.dot(R[i],D_n))\n",
    "\n",
    "            # log likelihood function to minimize \n",
    "            logL=logL+row*np.log(2*np.pi)+2*np.log(np.linalg.det(D_n))+np.log(np.linalg.det(R[i]))+np.matmul(self.returns[i], (np.matmul( np.linalg.inv(R[i]), self.returns[i].T)))\n",
    "            \n",
    "\n",
    "        return  logL  # we change the sign to minimize the function\n",
    "        \n",
    "    def m_garch_opt(self):\n",
    "        \"optimization of the parameters a and b minimzing the log-likelihood function\\\n",
    "        returning estimated parameters\"\n",
    "        \n",
    "        constraint=lambda parameters: (1-1e-8)-parameters[0]-parameters[1] \n",
    "        con={'type':'ineq','fun':constraint}\n",
    "        opt_params=scipy.optimize.minimize(self.m_garch_loglikilihood,(0.3,0.6),bounds=((1e-10,1),(1e-10,1)),constraints=con)\n",
    "        return opt_params.x\n",
    "\n",
    "    def m_garch_covariance(self,parameters):\n",
    "        \"give us the variance covariance matrix on the last day of our data sample\\\n",
    "        when we want to calculate volatility and Var\"\n",
    "        \n",
    "        a=parameters[0]\n",
    "        b=parameters[1]\n",
    "        row,col=self.returns.shape\n",
    "        S=self.m_garch_vol() # creating the S matrix with all the estandard deviations\n",
    "        Q_bar=np.cov(self.returns.reshape(col,row))#unconditional covariance .We approximate it by the sample covariance\n",
    "        \n",
    "        Q=np.zeros((row,col,col))\n",
    "        R=np.zeros((row,col,col))\n",
    "        H=np.zeros((row,col,col))\n",
    "\n",
    "\n",
    "        Q[0]=np.matmul(self.returns[0].T,self.returns[0])   \n",
    "\n",
    "        for i in range(1,row): #for each day\n",
    "            D_n=np.diag(S[i,:])\n",
    "            D_n_inv=np.linalg.inv(D_n)\n",
    "            resd=D_n_inv*self.returns[i,:].T  #calculating the standardized residuals\n",
    "            Q[i]=(1-a-b)*Q_bar+a*resd*resd.T +b*Q[i-1]      #calculating matrix Q\n",
    "            Q_star_inv=np.linalg.inv(np.sqrt(np.diag(np.diag(Q[i]))))\n",
    "            R[i]=np.dot(Q_star_inv,np.dot(Q[i],Q_star_inv))\n",
    "            H[i]=np.dot(D_n,np.dot(R[i],D_n))\n",
    "        return H[-1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that will call different methods to calculate portfolio volatility  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortVolatility:\n",
    "    \"\"\" This class implement different methods to calculate portfolio volatility.\n",
    "    Take as inputs the returns series, the weights, an a string with the name of the \n",
    "    method we want to use for the estimation.self.portVolatility is an attrivute of the class that return the portfolio \n",
    "    volatility estimated with the method we specified.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,returns,weights,method):\n",
    "        self.method=method.lower()\n",
    "        self.returns=returns\n",
    "        self.weights=weights\n",
    "        \n",
    "\n",
    "        if not isinstance(self.returns,(pd.Series,pd.DataFrame)):\n",
    "            raise TypeError('returns must be a pandas dataframe or series')\n",
    "        \n",
    "        if self.method=='eqwma':\n",
    "            \"\"\"Estimation of the portfolio volatility with the EQWMA method\"\"\"\n",
    "            if isinstance(self.returns,pd.Series):\n",
    "                self.portVolatility=np.std(self.returns)\n",
    "            else:\n",
    "                covMatrix=self.returns.cov()\n",
    "                self.portVolatility=np.sqrt(np.dot(self.weights.T,np.dot(covMatrix,self.weights)))\n",
    "            \n",
    "        elif self.method=='ewma':\n",
    "            \"Calling the method 'EWMA' for the estimation of portfolio volatility\"\n",
    "            self.portVolatility=self.EWMA()\n",
    "            \n",
    "        else:  \n",
    "            \"\"\"Calculates the portfolio volatility with GARCH(1,1) or DCC m-GARCH(1,1) method.\"\"\"\n",
    "            if isinstance(self.returns,pd.Series):\n",
    "                garch=Garch1_1(returns)\n",
    "                opt_params=garch.garch_opt()  # getting the optimal parameters c,a,b\n",
    "                self.portVolatility=np.sqrt(garch.garch_variance(opt_params)[-1])\n",
    "            else:\n",
    "                garch=DCC_garch(returns)\n",
    "                opt_params= garch.m_garch_opt()\n",
    "                covMatrix=garch.m_garch_covariance(opt_params)\n",
    "                self.portVolatility=np.sqrt(np.dot(self.weights.T,np.dot(covMatrix,self.weights)))\n",
    "                         \n",
    "            \n",
    "    def EWMA(self):\n",
    "        \"\"\"method that estimates the portfolio volatility with the EWMA approach\"\"\"\n",
    "        \n",
    "        lambda_=0.94\n",
    "        squaredReturns=np.power(self.returns,2)\n",
    "        if isinstance(self.returns,(pd.Series)):\n",
    "            variance=np.var(self.returns)\n",
    "            for i in range(1,len(self.returns)):\n",
    "                var=lambda_*variance+(1-lambda_)*squaredReturns.iloc[i-1]\n",
    "                variance=var \n",
    "            return np.sqrt(variance)\n",
    "\n",
    "        else:\n",
    "            covMatrix=self.returns.cov() # we initialize the variances and covariances as the sample variances and covariances at n=0\n",
    "            row,col=squaredReturns.shape  #number of data points and assets \n",
    "            \n",
    "            for i in range(1,row): # for each day in the sample\n",
    "                for j in range(col):# for each asset in the portfolio\n",
    "\n",
    "                    #updating variances for each asset\n",
    "                    updatedVar=lambda_*covMatrix.iloc[j,j]+(1-lambda_)*squaredReturns.iloc[i-1]\n",
    "                    covMatrix.iloc[j,j]=updatedVar[j]\n",
    "\n",
    "                    #updating covariances\n",
    "                    cov=covMatrix.iloc[j,covMatrix.iloc[j,:].name!=covMatrix.columns]\n",
    "                    updatedCov=lambda_*cov+(1-lambda_)*self.returns.loc[i-1,cov.index]*self.returns.loc[i-1,covMatrix.iloc[j,:].name]\n",
    "                    covMatrix.loc[covMatrix.iloc[j,:].name,covMatrix.iloc[j,:].name!=covMatrix.columns]=updatedCov\n",
    "            portVol=np.sqrt(np.dot(self.weights.T,np.dot(covMatrix,self.weights)))\n",
    "            return portVol\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VALUE AT RISK MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This class implement different value at risk models for a single and multi-asset portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueAtRisk():\n",
    "    def __init__(self,data,returns,alpha,P0,weights,h,model):\n",
    "        \n",
    "        \n",
    "        \"\"\"This class is going to calculate the h-day (1-alpha) Var of a portfolio. It takes as inputs 'data'\n",
    "        which is the series of the closing prices of the assets included in the eportfolio,the series of returns,'alpha' or \n",
    "        significante level, the initial investment 'P0', the weights or proportion of the investment money allocated to each asset,\n",
    "        'h' or the risk horizon and a string witht he name of the model we want to use for Var estimation.\"\"\"\n",
    "        \n",
    "        self.model=model\n",
    "        self.data=data\n",
    "        self.returns=returns\n",
    "        self.alpha=alpha\n",
    "        self.P0=P0\n",
    "        self.weights=weights\n",
    "        self.h=h\n",
    "        \n",
    "        #checking for valid format of data \n",
    "        if not isinstance(self.data,(pd.Series,pd.DataFrame)) or\\\n",
    "        not isinstance(self.returns,(pd.Series,pd.DataFrame)):\n",
    "            raise TypeError('data and returns must be a pandas dataframe or series')\n",
    "            \n",
    "        # case that a pandas dataframe contains only one column. We pass it to pandas.Series\n",
    "        if isinstance(self.returns,pd.DataFrame) and self.returns.shape[1]==1:\n",
    "            self.returns=self.returns.squeeze()\n",
    "        elif isinstance(self.data,pd.DataFrame) and self.data.shape[1]==1:\n",
    "            self.data=self.data.squeeze()\n",
    "            \n",
    "        \n",
    "        if self.model=='hs':\n",
    "            self.Var= self.historicalSimVar()[1]\n",
    "        \n",
    "        elif self.model=='mc':\n",
    "            self.Var=self.MCVar()\n",
    "            \n",
    "        else:\n",
    "            self.Var=self.parametricVar()\n",
    "        \n",
    "\n",
    "    def historicalSimVar(self):\n",
    "        \"\"\" This method computes the h-day (1-alpha) Var with historical simulation method \"\"\"\n",
    "\n",
    "        df=self.data.div(self.data.shift(1)) #calculating the change in prices of each asset\n",
    "        df.dropna(inplace=True)\n",
    "        value_port=df*self.P0*self.weights #calculating the value of porfolio for each scenario\n",
    "        \n",
    "        if isinstance(self.data,pd.Series):\n",
    "            df=df.to_frame()\n",
    "            df['Portafolio Value']=value_port\n",
    "        else:\n",
    "            df['Portafolio Value']=value_port.sum(axis=1)\n",
    "\n",
    "        df['Loss']=self.P0-df['Portafolio Value'] \n",
    "        df.sort_values(by=['Loss'],ascending=False,inplace=True) # sorting losses. Gains are negative losses\n",
    "        \n",
    "        return df.Loss, df.Loss.quantile(q=1-self.alpha,interpolation='higher')*np.sqrt(self.h)\n",
    "    \n",
    "    \n",
    "    def parametricVar(self):\n",
    "        \"\"\"This method computes the h-day (1-alpha) Var with parametric method and assuming normal distribution \"\"\"\n",
    "        portVol=PortVolatility(self.returns,self.weights,self.model).portVolatility\n",
    "        meanReturns=0\n",
    "        portMean=0 #np.sum(self.weights.T*meanReturns) \n",
    "        stat=norm.ppf(1-self.alpha)\n",
    "        Var=(stat*portVol-portMean)*self.P0*np.sqrt(self.h) \n",
    "        return Var\n",
    "    \n",
    "    def MCVar(self):\n",
    "        \"\"\"Method that computes the h-day (1-alpha) Var with Monte Carlo \"\"\"\n",
    "         \n",
    "        n_sims=10000 # we can change the number of simulations \n",
    "        meanReturns=0\n",
    "        \n",
    "        if isinstance(self.returns,pd.Series):\n",
    "            vol=np.std(self.returns)\n",
    "             # independent random standard normal vectors for risk factor \n",
    "            Z=norm.ppf(np.random.uniform(size=n_sims))\n",
    "            # calculate the portfolio daily profit and losses\n",
    "            portLosses=meanReturns+Z*vol*self.P0*np.sqrt(self.h/252) \n",
    "            \n",
    "        else:\n",
    "            #we calculate the covariance matrix using the equally weighted method, but we can use the other method to calculate the covariance matrix\n",
    "            covMatrix=self.returns.cov()\n",
    "            #cholesky decomposition of the returns covariance matrix\n",
    "            L = cholesky(covMatrix, lower=True) \n",
    "            #independent random standard normal vectors for each risk factor \n",
    "            Z=norm.ppf(np.random.uniform(size=(n_sims,len(self.weights))))\n",
    "            #transformation of the independent standard normal vectors to correlated multivariate normal vectors \n",
    "            simReturns=np.array(meanReturns)+np.dot(Z,L)\n",
    "            # simulating the portfolio gains and losses\n",
    "            portLosses=np.sum(simReturns*self.weights, axis=1)*self.P0 \n",
    "            \n",
    "        loss=pd.Series(portLosses)\n",
    "        loss.sort_values(ascending=False,inplace=True) # sorting losses. Gains are negative losses\n",
    "        Var=loss.quantile(q=1-self.alpha,interpolation='higher')*np.sqrt(self.h)\n",
    "        return Var\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL VALIDATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a backtest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelValidation():\n",
    "    \"\"\"This class implement different methods and tests to validate the performance of our models\"\"\"\n",
    "    \n",
    "    def __init__(self,data,h,alpha,P0,weights,model,testSize=0.4):\n",
    "        self.data=data\n",
    "        self.h=h\n",
    "        self.P0=P0\n",
    "        self.alpha=alpha\n",
    "        self.weights=weights\n",
    "        self.model=model\n",
    "        self.percentTestSample=testSize\n",
    "  \n",
    "    def backTest(self):\n",
    "        \"\"\"Method that implement backtesting of our models calculating the 1-day Var for each day in the test data and \n",
    "        comparing them with the actual profits and losses. Returns a dataframe containing three columns:the actual profit and losses,\n",
    "        the 1-day (1-alpha) Var and the a column of exceedances or column indicating if the actual losses/gains are higher than Var (1) or lower (0).It also returns the \n",
    "        total numbe of exceedances\"\"\"\n",
    "        \n",
    "        dataSize=len(self.data)\n",
    "        testSampleStart=int(dataSize-dataSize*self.percentTestSample) # observation where the test sample starts and the estimation sample ends\n",
    "\n",
    "        #calculating the realized profits and losses. \n",
    "        returnsTest=self.data.iloc[testSampleStart:].pct_change()\n",
    "        returnsTest.dropna(inplace=True)\n",
    "\n",
    "        if isinstance(returnsTest,pd.DataFrame):\n",
    "            realizedLosses=np.sum(returnsTest*self.P0*self.weights,axis=1)\n",
    "        else: \n",
    "            realizedLosses=returnsTest*self.P0*self.weights\n",
    "        realizedLosses=realizedLosses.reset_index(drop=True)\n",
    "        #rolling window aproach: Estimation of Var for each of the days in the test sample.The estimation sample size is the same \n",
    "        #as we apply the rolling window approach.\n",
    "        VarTest=np.full(shape=(len(realizedLosses)),fill_value=0.0) #place holder for Vars\n",
    "\n",
    "        for i in range(len(realizedLosses)):\n",
    "\n",
    "            estimationData=self.data.iloc[i:testSampleStart+i] \n",
    "            estimationReturns= estimationData.pct_change()\n",
    "            estimationReturns.dropna(inplace=True)\n",
    "            estimationReturns=estimationReturns.reset_index(drop=True)\n",
    "\n",
    "            Var=ValueAtRisk(estimationData,estimationReturns,self.alpha,self.P0,self.weights,self.h,self.model).Var\n",
    "\n",
    "            # so far we express Var with positive sing but for comparing with \n",
    "            #the realised losses we change the sign. Losses are negative and gains are positive\n",
    "            VarTest[i]=-Var \n",
    "        df=pd.DataFrame({'Realized Losses':realizedLosses,'Var':pd.Series(VarTest)})\n",
    "        df['Exceedances']=df.apply(lambda x:1 if x['Realized Losses']<x['Var'] else 0, axis=1) \n",
    "        exceedances=np.sum(df['Exceedances'])\n",
    "        return df,exceedances\n",
    "\n",
    "    def exactBinomCI(self,p,df,epsilon=0.05):\n",
    "         \n",
    "        \"\"\"Estimating the lower and upper bound for the binomial confidence interval.\n",
    "        'p' is the expected proportion of exceedances that equal to alpha,'n' is sample size,\n",
    "        1-'epsilon' is the confidence that the expected proportion is in the interval.\"\"\"\n",
    "        n=len(df)\n",
    "        lower=binom.ppf(epsilon/2,n,p)\n",
    "        upper=binom.ppf(1-(epsilon/2),n,p)\n",
    "        return lower,upper\n",
    "    \n",
    "\n",
    "    def ind_test(self,df,epsilon=0.05):\n",
    "        \"\"\"Computes the independence test. It takes as inputs the dataframe output in the backtesting step and the \n",
    "        significance level (epsilon) for the hipothesis testing. It returns 'independent' if we can not reject the null \n",
    "        HO that the exceedance in each day are independent of the past at the SL, otherwise the result is not independet\"\"\"\n",
    "        n=len(df)\n",
    "        n00=0\n",
    "        n01=0\n",
    "        n10=0\n",
    "        n11=0\n",
    "        for i in range(1,n-1):\n",
    "            if df['Exceedances'].iloc[i-1]==0 and df['Exceedances'].iloc[i]==0:\n",
    "                n00+=1\n",
    "            elif df['Exceedances'].iloc[i-1]==0 and df['Exceedances'].iloc[i]==1:\n",
    "                n01+=1\n",
    "            elif df['Exceedances'].iloc[i-1]==1 and df['Exceedances'].iloc[i]==0:\n",
    "                n10+=1\n",
    "            else: n11+=1\n",
    "\n",
    "        p01=n01/(n00+n01)\n",
    "        p11=n11/(n10+n11)\n",
    "        #probability of having a failure\n",
    "        pUC=(n01+n11)/(n00+n01+n10+n11)\n",
    "\n",
    "        if 0 in [n00,n01,n10,n11]:\n",
    "            L=1\n",
    "            if n00==0 or n01==0:\n",
    "                L1=1\n",
    "            else: L1=n00*np.log(1-p01)+n01*np.log(p01)\n",
    "            \n",
    "            if n10==0 or n11==0:\n",
    "                L2=1\n",
    "            else: L2=n10*np.log(1-p11)+n11*np.log(p11)\n",
    "            lnRatio=L-L1-L2\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            L=(n00+n10)*np.log(1-pUC)+(n01+n11)*np.log(pUC)\n",
    "            L1=n00*np.log(1-p01)+n01*np.log(p01)\n",
    "            L2=n10*np.log(1-p11)+n11*np.log(p11)\n",
    "            lnRatio=L-L1-L2\n",
    "      \n",
    "        if -2*lnRatio>chi2.ppf(1-epsilon, df=1): #this p is the sl for the test too. we can change it \n",
    "            #h0: exceedances are independent at sl \n",
    "            return 'independent'\n",
    "        else:\n",
    "            #h1: exceedance are not independent at sl\n",
    "            return 'not independent'\n",
    "\n",
    "            \n",
    "       \n",
    "    def UCT(self,p,df,epsilon=0.05):\n",
    "        \"\"\"Performs the unconditional coverage test to compute how well the model estimates the actual portfolio risk.\n",
    "        Takes as input p=alpha or probability of exceedance, the df output ffrom the backtesting step, and the significance \n",
    "        level (epsilon). The outputs is the result of the hipothesis testing: model no well calibrated if we have evidence \n",
    "        for rejecting the H0 that the exceedances are close to the expected value. Otherwise,we cannot reject the HO and\n",
    "        we have evidence to conclude our model is well calibrated. \"\"\"\n",
    "        n=len(df)\n",
    "        n1=np.sum(df['Exceedances']) # number exceedances\n",
    "        n0=n-n1\n",
    "        lnRatio=n1*np.log(p)+n0*np.log(1-p)-n1*np.log(n1/n) #n1/n= observed proportion of exceedances\n",
    "        if -2*lnRatio>chi2.ppf(1-epsilon, df=1): # this p is the significance level for the h test too\n",
    "            return 'Model no well calibrated'\n",
    "        else:\n",
    "            return 'Model well calibrated'\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebad247e5534c1ba5b14ab8cc72fe2532bc9dd980a9cee8d1951b26de076f558"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
